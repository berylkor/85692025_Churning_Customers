# -*- coding: utf-8 -*-
"""85692025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DV300SQaHEeW2XslcvY2SFd8mr7VGEwK

# **Import Datasets**
"""

# Import relevenat libraries
import pandas as pd
import os
import sklearn
import numpy as np
import pandas as pd
from sklearn import tree, metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from google.colab import drive
drive.mount('/content/drive')

!pip install pymal h5py

churning_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Assignment 3/datasets_13996_18858_WA_Fn-UseC_-Telco-Customer-Churn.csv')

churning_data.head(5)

churning_data.info()

"""# **Relevant Features**"""

# drop id from dataframe
churning_data.drop('customerID', axis = 1, inplace = True)

churning2 = churning_data

label_encoder = LabelEncoder()

Y = churning_data['Churn'] # Move churn, the target column into a separate variable
encoded_Y = label_encoder.fit_transform(Y) # encode churn in Y
encoded_Y

churning_data.drop('Churn', axis = 1, inplace = True) # drop churn from the dataframe
churning_data.info()

# Replace blank space in the total charges column with nan values that can be imputed
churning_data['TotalCharges'] = churning_data['TotalCharges'].replace(r'^\s*$', np.nan, regex=True)
churning_data.info()

churning_data['TotalCharges'] = churning_data['TotalCharges'].fillna(0).astype(float) # impute the nan values in total charge column
churning_data.info()

numeric_churning_data = churning_data.select_dtypes(include=['int', 'float']).columns # separate the numeric columns
numeric_churning = churning_data[numeric_churning_data]
numeric_churning.info()

sc = StandardScaler() # variable for scaler

# scaled_numeric = sc.fit_transform(numeric_churning) # scale numeric data
# scaled_numeric_data = pd.DataFrame(scaled_numeric, columns = numeric_churning.columns) # move the scaled data into a new dataframe
# scaled_numeric_data.head(10)

object_churning_data = churning_data.select_dtypes(exclude=['int', 'float']).columns # select the object columns
object_churning = churning_data[object_churning_data] # move the object columns into a new variable
object_churning.info()

encoded_object = pd.DataFrame() # new dataframe for encoded data
for n in object_churning.columns:
  encoded_object[n] = label_encoder.fit_transform(object_churning[n]) # encode the data and move into the dataframe
encoded_object.info()

# scaled_object = sc.fit_transform(encoded_object) # scale the object data
# scaled_object_data = pd.DataFrame(encoded_object, columns = encoded_object.columns) # move the scaled data into the dataframe
# scaled_object_data.head(10)

X = pd.DataFrame() # new dataframe for features
X = pd.concat([encoded_object, numeric_churning], axis = 1) # combine the numeric and object data
X.info()

X.head(5)

rf = RandomForestClassifier(n_estimators=100, random_state=42) # variable for random forest classifier

rf.fit(X, Y)

feature_name = X.columns
feature_importance = rf.feature_importances_ # feature importance

sorted_feature_importance = pd.DataFrame({'Feature':feature_name,'Importance':feature_importance}) # dataframe with feature importance
sorted_feature_importance = sorted_feature_importance.sort_values(by='Importance', ascending=False) # features sorted in order of importance
sorted_feature_importance

first_eleven_features = sorted_feature_importance['Feature'].values[:11] # first eleven most important features
first_eleven_features

X = X[first_eleven_features]
X.info()

# scaled_numeric = sc.fit_transform(numeric_churning) # scale numeric data
# scaled_numeric_data = pd.DataFrame(scaled_numeric, columns = numeric_churning.columns) # move the scaled data into a new dataframe
# scaled_numeric_data.head(10)

Scaled_X = sc.fit_transform(X)
Scaled_X = pd.DataFrame(Scaled_X, columns = X.columns)
X.head(10)

"""# **Exploratory Data Analysis**"""

import seaborn as sb

features_to_drop = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling'] # list of non relevant features
object_churning.drop(features_to_drop, axis = 1, inplace = True) # drop the features from the dataframe since they are not relevant features
object_churning.info()

numeric_churning.drop('SeniorCitizen', axis = 1, inplace = True) # drop Senior Citizen from the dataframe since it is not a relevant feature
numeric_churning.info()

for i in numeric_churning:
  sb.boxplot(x= Y, y = i, data = numeric_churning)
  plt.title(f'Box plot of {i} by Churn')
  plt.show()

"""For the boxplot


*   The median total charges for people churning is around 500 while that of those not churning is closer to 2000
*   For monthly charges the meadian is slighlty higher than 60 for those not churning and about 75 for those churning
*   The medain tenure for customers churning is 10 while that of those not churning is 40



"""

figure, axis = plt.subplots(2,4, figsize = (20,15))
sb.countplot(x= 'gender', data = object_churning, hue = Y, ax = axis[0,0], palette = 'bright').set(xlabel = 'Gender')
sb.countplot(x= 'InternetService', data = object_churning, hue = Y, ax = axis[0,1], palette = 'muted').set(xlabel = 'InternetService')
sb.countplot(x= 'OnlineSecurity', data = object_churning, hue = Y, ax = axis[0,2], palette = 'pastel').set(xlabel = 'OnlineSecurity')
sb.countplot(x= 'OnlineBackup', data = object_churning, hue = Y, ax = axis[0,3], palette = 'copper').set(xlabel = 'OnlineBackup')
sb.countplot(x= 'DeviceProtection', data = object_churning, hue = Y, ax = axis[1,0], palette = 'Accent').set(xlabel = 'DeviceProtection')
sb.countplot(x= 'TechSupport', data = object_churning, hue = Y, ax = axis[1,1], palette = 'binary').set(xlabel = 'TechSupport')
sb.countplot(x= 'Contract', data = object_churning, hue = Y, ax = axis[1,2], palette = 'spring').set(xlabel = 'Contract')
sb.countplot(x= 'PaymentMethod', data = object_churning, hue = Y, ax = axis[1,3], palette = 'winter').set(xlabel = 'PaymentMethod')
plt.tight_layout()
plt.show()

"""From the graphs


*   There isn't a significant difference between churn for males and females. The males churns slightly less than the women
*   Customers with access to fiber optics churn significantly more than those with DSL and those with no internet service churn the least
*  Customers with no online security, backup and device protection churn far more than with online security.
*   Customers that are unable to utilise tech support churn much more than those that do have access to tech support
*   Customers with a month to month contract seem to churn a lot then there is a drastic decrease in churning for the one year contract with people with a two year contract churning very little
*   Customers who pay by electronic checks churn more than those who use mailed checks followeed by transfers and credit and debit cards.

# **Artificial neural network training and testing**
"""

import keras
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.optimizers import Adadelta
from keras.optimizers import Adam
from keras.optimizers import Adagrad
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, encoded_Y, test_size=0.2, random_state=42)
Xvalidate, Xtest, Yvalidate, Ytest = train_test_split(Xtest, Ytest, test_size = 0.5, random_state = 42)

Xtrain.shape

Ytrain.shape

# Functional API model
input_layer = Input(shape=(Xtrain.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)
hidden_layer_4 = Dense(12, activation='relu')(hidden_layer_3)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_4)
functional_model = Model(inputs=input_layer, outputs=output_layer)

functional_model.compile(optimizer=Adadelta(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
functional_model.fit(Xtrain, Ytrain, epochs=70, batch_size=32, validation_data=(Xvalidate, Yvalidate), verbose= 1) # define the model

_, accuracy = functional_model.evaluate(Xtrain, Ytrain)
accuracy*100

loss, accuracy = functional_model.evaluate(Xtest, Ytest)
print(f'Test Loss: {loss:.4f}') # model loss function
print(f'Test Accuracy: {accuracy*100:.4f}') # model accuracy

Ypred = functional_model.predict(Xtest)

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Ytest, Ypred) # AUC for model without grid search
auc

!pip install tensorflow scikeras scikit-learn

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adadelta
from scikeras.wrappers import KerasClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import load_model
from sklearn.datasets import make_classification
import joblib
import pickle

def model_create(hidden_units = 32, optimizer = 'adam' ):
  input_layer = Input(shape=(Xtrain.shape[1],))
  hidden_layer1 = Dense(hidden_units, activation = 'relu')(input_layer)
  hidden_layer2 = Dense(32, activation = 'relu')(hidden_layer1)
  hidden_layer3 = Dense(32, activation = 'relu')(hidden_layer2)
  output_layer = Dense(1,activation = 'sigmoid')(hidden_layer3)
  functional_model = Model(inputs = input_layer, outputs = output_layer)
  functional_model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])
  return functional_model

functional_model = KerasClassifier(model = model_create, epochs = 10, batch_size = 32, verbose = True, hidden_units = 32)

PARAMETERS = {
    'optimizer': ['rmsprop', 'adam'],
    'random_state': [28, 32, 64],
    'batch_size': [28, 32, 64]
}

cv = StratifiedKFold(n_splits = 5)
functional_cv = GridSearchCV(estimator = functional_model, param_grid= PARAMETERS, cv = cv, scoring = 'accuracy')
result = functional_cv.fit(Xtrain, Ytrain,  validation_data = (Xvalidate,  Yvalidate), verbose = 1)

print('Best accuracy: %f using %s' % (functional_cv.best_score_, functional_cv.best_params_))

Ypred = functional_cv.predict(Xtest)
Ypred

accuracy_score(Ytest, Ypred) # accuracy for the model with Grid Search

auc = roc_auc_score(Ytest, Ypred) # AUC model for the model with Grid Search
auc

"""# **Exporting model, scalar and encoder**"""

func_model = model_create(28, 'rmsprop') # export functional model
func_model.save('func_model.h5')

with open('scalar.pkl', 'wb') as file: # export scalar
  pickle.dump(sc, file)

with open('encoder.pkl', 'wb') as file: # export encoder
  pickle.dump(label_encoder, file)

